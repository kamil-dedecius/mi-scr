{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d46b969f-fe24-4efb-8d75-078b63ccf223",
   "metadata": {},
   "source": [
    "# Téma: LSTM - Long Short-Term Memory\n",
    "\n",
    "V poslední přednášce zlehka nahlédneme do neuronových sítí, konkrétně do LSTM. Jelikož celá problematika by vydala na samotný semestr, nebudeme se zabývat podrobnostmi, ale jen filosofií."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477dbf22-bc8e-49d9-9da3-d2e8a51ac59c",
   "metadata": {},
   "source": [
    "## Proč (spíš ne) RNN?\n",
    "\n",
    "Rekurentní neuronové sítě vzniknou, zjednodušeně řečeno, následujícím způsobem: Uvažujme nejprve jednoduchou neuronovou síť, která má na základě vstupu $X_t$ predikovat $Y_t$. Nechť obsahuje nějaké váhy, aktivační funkci, potenciálně další vrstvy a další prvky tak, jak jsme zvyklí. Pro jednoduchost si to znázorníme následovně:\n",
    "\n",
    "<center>\n",
    "<img src=\"img/rnn-1.png\" width=\"60%\" />\n",
    "</center>\n",
    "    \n",
    "Nyní chceme zajistit, aby do predikce $Y_t$ vstupovala i znalost z minule. Elegantním způsobem, jak to udělat, je vložit zpětnou smyčku z výstupu zpět do vstupu. Pokud takovou síť \"rozbalíme\" (*unfold*), pak to bude vypadat takto:\n",
    "\n",
    "<center>\n",
    "<img src=\"img/rnn-2.png\" width=\"60%\" />\n",
    "</center>\n",
    "\n",
    "Znalost se nám tedy rekurentně propaguje. Pokud takovou síť začneme učit, narazíme následující složitost: Multiplikativní váha $w$ propojující sítě se s každou novou vrstvou umocní. Pokud by tedy platilo $w>1$, budeme mít problémy s optimalizací kvůli explozivnímu charakteru gradientu. Naopak, pokud bude váha $w<1$, půjde gradient exponenciálně k nule (tzv. *vanishing gradient problem*). Ladění $w$ tedy představuje zásadní optimalizační problém. Navíc toto propojení v RNN neumožňuje elegantně přenést dlouhodobou paměť, např. mezi časem $t-5$ a $t$, neboť informace musí vždy \"probublat\" přes všechny mezilehlé vrstvy.\n",
    "\n",
    "Pro úplnost dodejme, že RNN se typicky zobrazují následovně (zdroj wikipedia.org)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735cbb2b-eaed-4950-8e00-73578954d4e5",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"img/rnn-unfold.png\" width=\"70%\" />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187f8424-2d8a-4ee3-ae41-c73b36f9a7ad",
   "metadata": {},
   "source": [
    "## LSTM\n",
    "\n",
    "LSTM - Long Short-Term Memory - je alternativa k RNN vhodná pro data s charakterem časových řad. Jejich vznik se datuje do druhé poloviny 90. let, hlavními autory byli Sepp Hochreiter, Jürgen Schmidhuber, Felix Gers a Jürgen Schmidhuber. Úspěšné aplikace měly např. ve zpracování hlasového signálu, tedy tam, kde je k dispozici **hodně trénovacích dat**. Trénování typicky využívá gradientní metody spolu se zpětnou propagací.\n",
    "\n",
    "### Architektura buňky\n",
    "Základní LSTM buňka se sestává z\n",
    "- brány zapomínání (*forget gate*) - na základě porovnání se současnou (krátkodobou) informací rozhoduje, jaká část dlouhodobé informace (stavu buňky) bude zahozena. Typicky využívá sigmoidu, jejímž výstupem je číslo mezi 0 a 1.\n",
    "- vstupní brány (*input gate*) - rozhoduje, jaká míra nové informace bude vložena do informace dlouhodobé, tj. do stavu.\n",
    "- výstupní brány (*output gate*) - \"míchá\" dlouhodobou informaci (stav) s krátkodobou a poskytuje výstup z buňky jako hodnotu predikce.\n",
    "\n",
    "Podívejme se na buňku blíže:\n",
    "- světle modrý blok je brána zapomínání\n",
    "- zelený a fialový blok dávají dohromady vstupní bránu\n",
    "- oranžový a šedý blok tvoří výstupní bránu\n",
    "- LT je long-term paměť, tj. stav buňky\n",
    "- ST je short-term paměť, tj. vnitřní stav buňky. Slouží jako výstup buňky"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83db0f2e-408f-4232-b422-7f1731081359",
   "metadata": {},
   "source": [
    "<img src=\"img/lstm.png\" width=\"100%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2ab477-1f96-4fbb-a94e-13f699e1ffe4",
   "metadata": {},
   "source": [
    "#### **Brána zapomínání**\n",
    "Zkuste sami rozklíčovat, jak tato brána funguje:\n",
    "<img src=\"img/lstm-forget.png\" width=\"100%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9942b37-2dcc-4616-b37b-a3e7125d322f",
   "metadata": {},
   "source": [
    "#### **Vstupní brána**\n",
    "Zkuste sami rozklíčovat, jak tato brána funguje:\n",
    "<img src=\"img/lstm-forget-memory.png\" width=\"100%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9953532b-039a-485a-9648-59ae8c5b5d61",
   "metadata": {},
   "source": [
    "#### **Výstupní brána**\n",
    "Zkuste sami rozklíčovat, jak tato brána funguje:\n",
    "<img src=\"img/lstm.png\" width=\"100%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695b8c97-9bdb-4810-ac4f-772bca1a365a",
   "metadata": {},
   "source": [
    "### Varianty LSTM\n",
    "\n",
    "- Peephole LSTM - upgrade od Gerse a Schmidhubera, následovaný mnoha dalšími. Brány mají přístup k informaci dlouhodobé paměti (stavu).\n",
    "- Gated Recurrent Unit (GRU) - Cho et al. - kombinuje brány zapomínání a vstupu do jedné updatovací brány. Dále kombinuje stav a vnitřní stav. \n",
    "- Depth-Gated RNN, Clockwork RNN, Grid LSTM a spousty dalších...\n",
    "\n",
    "V roce 2015 vyšla studie (Greff et al.), která analyzovala populární varianty LSTM sítí se závěrem, že jejich výsledky jsou víceméně srovnatelné. V tomtéž roce porovnal Jozefowicz a jeho tým srovnání obrovského množství RNN. Výsledkem bylo, že některé RNN dávají lepší výsledky, než LSTM. V určitých aplikacích :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
